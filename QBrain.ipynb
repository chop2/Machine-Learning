{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Activation,Dense,Flatten,Convolution2D\n",
    "from collections import deque\n",
    "class QBrain:\n",
    "    def __init__(self,num_act = 18,capacity = 10000,esispde = 1000,batch_size = 32,esp=0.01,gama = 0.7):\n",
    "        self.ACTION = num_act\n",
    "        self.CAPACITY_MEM = capacity\n",
    "        self.ESISODE = esispde\n",
    "        self.batch_size = batch_size\n",
    "        self.esplion = esp\n",
    "        self.GAMA = gama\n",
    "        self.replayMem = deque(maxlen=self.CAPACITY_MEM)\n",
    "        self.OBSERVE = 10\n",
    "        self.SKIP_FRAME = 4\n",
    "        self.time_step=0\n",
    "    def createQNet(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32,(5,5),strides=(1,1),batch_input_shape=(self.batch_size,80,80,3),data_format='channels_last'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(MaxPool2D((2,2)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(Conv2D(64,(5,5),strides=(1,1)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(MaxPool2D((2,2)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(Conv2D(128,(5,5),strides=(1,1)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(MaxPool2D((2,2)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        \n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(1000,activation=Activation('relu')))\n",
    "        self.model.add(Dense(1000,activation=Activation('relu')))\n",
    "        self.model.add(Dense(self.ACTION,activation=Activation('relu')))\n",
    "        \n",
    "        self.model.compile(loss='mse',optimizer='sgd')\n",
    "                \n",
    "    def trainNet(self):\n",
    "        #fetch data from replayMem\n",
    "        replayItem = [self.replayMem[np.random.randint(len(self.replayMem))] for i in range(self.batch_size)]\n",
    "        minibatch_state = [iter[0] for iter in replayItem]\n",
    "        minibatch_action = [iter[1] for iter in replayItem]\n",
    "        minibatch_reward = [iter[2] for iter in replayItem]\n",
    "        minibatch_state_next = [iter[3] for iter in replayItem]\n",
    "        minibatch_terminal = [iter[4] for iter in replayItem]\n",
    "        \n",
    "        Q_values = self.model.predict_on_batch(minibatch_state_next)\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            if minibatch_terminal[i]:\n",
    "                y_batch.append(minibatch_reward[i])\n",
    "            else:\n",
    "                y_batch.append(minibatch_reward[i] + self.GAMA*(np.max(Q_values[i])))\n",
    "        rsp = self.model.train_on_batch(minibatch_state,y_batch)\n",
    "        print(rsp)\n",
    "    def getAction(self):        \n",
    "        rand_seed = np.random.random()\n",
    "        if rand_seed < self.esplion:\n",
    "            self.action = np.zeros((1,self.ACTION))\n",
    "            self.action[0,np.random.randint(self.ACTION)] = 1\n",
    "        else:\n",
    "            batch_state_list = []\n",
    "            for i in range(self.batch_size):\n",
    "                batch_state_list.append(self.currState)\n",
    "            batch_state = np.array(batch_state_list)\n",
    "            self.action = self.model.predict(batch_state)\n",
    "        best_row = [self.action[i,:].max() for i in range(self.action.shape[0])]        \n",
    "        return self.action[np.array(best_row).argmax(),:]\n",
    "    def setSequnceState(self,nextState,reward,terminate):\n",
    "        self.replayMem.append([self.currState,self.action,reward,nextState,terminate])\n",
    "        if self.time_step > self.OBSERVE:\n",
    "            if self.time_step % self.SKIP_FRAME == 0:\n",
    "                print(\"Enter Net\")\n",
    "                self.trainNet()\n",
    "        self.time_step += 1\n",
    "        self.currState = nextState        \n",
    "        \n",
    "    def initSequnceState(self,state):\n",
    "        self.action = np.zeros((1,self.ACTION))\n",
    "        self.action[0,np.random.randint(self.ACTION)] = 1\n",
    "        self.currState = state        \n",
    "        best_row = [self.action[i,:].max() for i in range(self.action.shape[0])]        \n",
    "        return self.action[np.array(best_row).argmax(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from ale_python_interface import ALEInterface\n",
    "import cv2\n",
    " \n",
    "def imgpreprocess(img,size=(80,80)):\n",
    "    img = cv2.resize(img,size)\n",
    "    return img\n",
    "\n",
    "agent = QBrain()\n",
    "agent.createQNet()\n",
    "game_path = b'F:/github/Arcade-GAME2/Alien.a26'\n",
    "ale = ALEInterface()\n",
    "ale.setInt(b'random_seed',123)\n",
    "ale.setBool(b'display_screen',True)\n",
    "ale.loadROM(game_path)\n",
    "legal_actins = ale.getLegalActionSet()\n",
    "w,h = ale.getScreenDims()\n",
    "screenData = np.empty((h,w,3),dtype=np.uint8)\n",
    "ale.getScreenRGB(screenData)\n",
    "init_state = imgpreprocess(screenData)\n",
    "action = agent.initSequnceState(init_state)\n",
    "total_reward = 0\n",
    "k = 0\n",
    "while k < 0:#not ale.game_over():\n",
    "    #reward = ale.act(np.random.randint(18))\n",
    "    reward = ale.act(action.argmax())    \n",
    "    ale.getScreenRGB(screenData)\n",
    "    next_state = imgpreprocess(screenData)\n",
    "    agent.setSequnceState(next_state,reward,False)\n",
    "    action = agent.getAction()\n",
    "    total_reward += reward\n",
    "    #print(\"action: \" , action.argmax(), \"   total_reward: \" , total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]] [['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c']] [[True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True], [True, False, False, True]]\n"
     ]
    }
   ],
   "source": [
    "aa = np.array([[1,2,3,4,5],['a','b','c'],[True,False,False,True]])\n",
    "aset = deque(maxlen=100)\n",
    "for i in range(10):\n",
    "    aset.append(aa)\n",
    "    \n",
    "a1 = [i[0] for i in aset]\n",
    "a2 = [i[1] for i in aset]\n",
    "a3 = [i[2] for i in aset]\n",
    "\n",
    "print(a1,a2,a3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
